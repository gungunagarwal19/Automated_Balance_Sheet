# -*- coding: utf-8 -*-
"""withamounts.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KcmL-ZzqhWy8Jw5_JDNHrX5vcoceruNb
"""

# ============================================
# üß© UPDATED INSPECTION CELL ‚Äî With Amount Simulation
# ============================================

import pandas as pd
import numpy as np
from pathlib import Path

# Step 1Ô∏è‚É£: Read the 'Main' sheet (skip first 2 rows)
excel_path = Path("with amounts.xlsx")
main_df = pd.read_excel(excel_path, sheet_name="Main", header=1)

# Step 2Ô∏è‚É£: Rename last two columns safely
col_names = list(main_df.columns)
col_names[-2] = "current_amount"
col_names[-1] = "prev_amount"
main_df.columns = col_names

# Step 3Ô∏è‚É£: Generate synthetic values with 5‚Äì35% difference
n = len(main_df)
np.random.seed(42)

# Create previous amounts between ‚Çπ1,00,000 and ‚Çπ10,00,000
main_df["prev_amount"] = np.random.uniform(1e5, 1e6, n).round(2)

# Generate random change percentage (5%‚Äì35%) and direction (+/-)
variation_pct = np.random.uniform(0.05, 0.35, n)
direction = np.random.choice([-1, 1], n, p=[0.4, 0.6])  # 60% increases
main_df["current_amount"] = (main_df["prev_amount"] * (1 + direction * variation_pct)).round(2)

# Step 4Ô∏è‚É£: Compute variance value (% difference)
main_df["variance_value"] = ((main_df["current_amount"] - main_df["prev_amount"]) / main_df["prev_amount"] * 100).round(2)

# Step 5Ô∏è‚É£: Confirm the structure
print("‚úÖ Columns detected in 'Main' sheet:\n")
for idx, col in enumerate(main_df.columns):
    print(f"{idx+1:02d}. {col}")

# Step 6Ô∏è‚É£: Show preview for verification
print("\nüìã Preview of first 5 rows (showing new amount fields):")
display(main_df.tail(5)[["current_amount", "prev_amount", "variance_value"]])

# ============================================
# üß© CELL ‚Äî Add Assigned Timeline & Review Dates
# ============================================

import pandas as pd
import numpy as np
from datetime import datetime, timedelta

# Assume current reporting date
current_date = datetime(2025, 11, 7)  # You can dynamically set this if needed

# Number of records
n = len(main_df)

# Step 1Ô∏è‚É£: Assign expected completion dates (randomly in the last 60‚Äì90 days)
np.random.seed(42)
assigned_offsets = np.random.randint(60, 90, n)  # assigned timeline window
main_df["assigned_date"] = [current_date - timedelta(days=int(x)) for x in assigned_offsets]

# Step 2Ô∏è‚É£: Assign actual review dates (some on time, some delayed)
# 70% completed on time, 30% delayed
delay_days = np.random.randint(-2, 20, n)  # -2 = early, 20 = up to 20 days delay
main_df["review_date"] = [
    assigned + timedelta(days=int(delay))
    for assigned, delay in zip(main_df["assigned_date"], delay_days)
]

# Step 3Ô∏è‚É£: Compute timeline deviation (in days)
main_df["timeline_deviation_days"] = (
    (main_df["review_date"] - main_df["assigned_date"]).dt.days
)

# Step 4Ô∏è‚É£: Add a categorical deviation flag
main_df["timeline_status"] = np.where(
    main_df["timeline_deviation_days"] > 0, "Delayed", "On Time"
)

# Step 5Ô∏è‚É£: Show confirmation summary
print("‚úÖ Assigned and Review Dates added successfully.\n")
print(f"Current Date: {current_date.strftime('%d-%b-%Y')}")
print(f"Rows updated: {n}\n")
print("üìä Timeline Summary:")
display(main_df[["assigned_date", "review_date", "timeline_deviation_days", "timeline_status"]].head(10))

print("\nüîπ Distribution of timeline status:")
print(main_df["timeline_status"].value_counts())

main_df

# ============================================
# üß© STEP ‚Äî Rename All Columns to Standard Names
# ============================================

import pandas as pd

# Work on a copy for safety
df = main_df.copy()

# Define new cleaned column names (matching the sequence you provided)
new_columns = [
    "bspl",                       # BS/PL
    "status",                     # Status
    "gl_acct",                    # G/L Acct
    "gl_account_number",          # G/L Account Number
    "main_head",                  # Main Head
    "sub_head",                   # Sub head
    "criticality",                # C/M/L
    "frequency",                  # Frequency
    "responsible_department",     # Responsible Department
    "department_spoc",            # Departement SPOC
    # Skipping mid columns automatically handled below
]

# To dynamically fill all columns up to the last known structure
all_cols = list(df.columns)

# --- Map the known last columns explicitly ---
# These are the new additions you mentioned
tail_cols_map = {
    "Type of Report": "type_of_report",
    "Analysis Requrired": "analysis_required",
    "Review Check point at ABEX": "review_check_point_at_abex",
    "current_amount": "current_amount",
    "prev_amount": "prev_amount",
    "variance_value": "variance_value",
    "assigned_date": "assigned_date",
    "review_date": "review_date",
    "timeline_deviation_days": "timeline_deviation_days",
    "timeline_status": "timeline_status"
}

# --- Step 1Ô∏è‚É£: Auto-generate a mapping dictionary ---
rename_dict = {}
for col in all_cols:
    clean = col.strip().replace("\n", " ").replace("  ", " ")
    if clean in tail_cols_map:
        rename_dict[col] = tail_cols_map[clean]
    else:
        # fallback to lower_snake_case auto-clean
        rename_dict[col] = (
            clean.lower()
            .replace(" ", "_")
            .replace("/", "_")
            .replace("(", "")
            .replace(")", "")
            .replace("%", "pct")
            .replace("-", "_")
        )

# --- Step 2Ô∏è‚É£: Rename all columns ---
df.rename(columns=rename_dict, inplace=True)

# --- Step 3Ô∏è‚É£: Verify result ---
print("‚úÖ Columns successfully standardized.\n")
print(f"Total columns: {len(df.columns)}\n")
for i, c in enumerate(df.columns, 1):
    print(f"{i:02d}. {c}")

# --- Step 4Ô∏è‚É£: Save back to main_df for next processing ---
main_df = df.copy()

print("\nüìã Preview of renamed data:")
display(main_df.head(3))

# ============================================
# üß© AUTOMATED REPORT GENERATION ‚Äî Full Analytics & Visualization
# ============================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
from pathlib import Path

df = main_df.copy()

# --- 1Ô∏è‚É£ Pending backup/supporting workings ---
def classify_working_status(text):
    if pd.isna(text) or str(text).strip() == "":
        return "Pending"
    t = str(text).lower()
    if any(k in t for k in ["no balance", "not applicable", "there shall not be any balances", "nil balance"]):
        return "No Work Required"
    if any(k in t for k in ["support", "pending", "required", "working required", "reclassification entries"]):
        return "Pending"
    return "Complete"

df["working_status"] = df["working_needed"].apply(classify_working_status)

# --- 2Ô∏è‚É£ Hygiene (Reviewed vs Pending Reviews) ---
df["flag_norm"] = df["flag_green___red"].astype(str).str.strip().str.lower().fillna("unknown")
reviewed_df = df[df["flag_norm"].str.contains("green", na=False)]
pending_review_df = df[df["flag_norm"].str.contains("red", na=False)]
unknown_df = df[~(df.index.isin(reviewed_df.index) | df.index.isin(pending_review_df.index))]

# --- 3Ô∏è‚É£ Timeline Deviation ---
df["timeline_status"] = np.where(df["timeline_deviation_days"] > 0, "Delayed", "On Time")

# --- 4Ô∏è‚É£ Major GL Variances ---
mean_var = df["variance_value"].abs().mean()
threshold = mean_var * 2
major_var_df = df[df["variance_value"].abs() > threshold]

# --- 5Ô∏è‚É£ Compute Summary Metrics ---
summary_data = {
    "Metric": [
        "Total GL Accounts",
        "Pending Backup / Supporting Workings",
        "Complete Workings",
        "No Work Required (Nil/NA)",
        "Reviewed GLs (Green)",
        "Pending Reviews (Red)",
        "Timeline Deviations (Delayed)",
        "Major GL Variances (>2√ó Mean Variance)"
    ],
    "Count": [
        len(df),
        len(df[df["working_status"] == "Pending"]),
        len(df[df["working_status"] == "Complete"]),
        len(df[df["working_status"] == "No Work Required"]),
        len(reviewed_df),
        len(pending_review_df),
        len(df[df["timeline_status"] == "Delayed"]),
        len(major_var_df)
    ]
}

summary_df = pd.DataFrame(summary_data)
print("‚úÖ Automated Reconciliation Report Summary:\n")
display(summary_df)

# --- 6Ô∏è‚É£ Visualization Section ---
plt.rcParams.update({'figure.max_open_warning': 0})

# A. Summary Overview
plt.figure(figsize=(10, 5))
plt.bar(summary_df["Metric"], summary_df["Count"], color="skyblue", edgecolor="black")
plt.xticks(rotation=30, ha="right")
plt.title("Automated Reconciliation Summary ‚Äî GL Accounts")
plt.tight_layout()
plt.show()

# B. Hygiene Pie Chart
flag_counts = pd.Series({
    "Reviewed (Green)": len(reviewed_df),
    "Pending Review (Red)": len(pending_review_df),
    "Unknown": len(unknown_df)
})
plt.figure(figsize=(5, 5))
flag_counts.plot(kind="pie", autopct="%1.1f%%", startangle=120, colors=["lightgreen", "salmon", "lightgrey"])
plt.title("GL Hygiene ‚Äî Reviewed vs Pending")
plt.ylabel("")
plt.show()

# C. Working Status Pie
work_counts = df["working_status"].value_counts()
plt.figure(figsize=(5, 5))
work_counts.plot(kind="pie", autopct="%1.1f%%", startangle=120)
plt.title("Working / Supporting File Status")
plt.ylabel("")
plt.show()

# D. Pending Uploads by Department
if "responsible_department" in df.columns:
    pend_dept = (
        df[df["working_status"] == "Pending"]
        .groupby("responsible_department")
        .size()
        .sort_values(ascending=False)
        .head(10)
    )
    if not pend_dept.empty:
        plt.figure(figsize=(8, 4))
        pend_dept.plot(kind="barh", color="orange")
        plt.gca().invert_yaxis()
        plt.title("Top 10 Departments ‚Äî Pending Workings")
        plt.xlabel("Count")
        plt.tight_layout()
        plt.show()

# E. Variance Distribution
if df["variance_value"].abs().sum() > 0:
    plt.figure(figsize=(8, 4))
    plt.hist(df["variance_value"], bins=30, color="lightblue", edgecolor="black")
    plt.axvline(threshold, color="red", linestyle="--", label=f"¬±{threshold:.2f}% Threshold")
    plt.title("Distribution of GL Variance (%)")
    plt.xlabel("% Variance")
    plt.legend()
    plt.tight_layout()
    plt.show()

# F. Timeline Delay Analysis
if "responsible_department" in df.columns:
    avg_delay = df.groupby("responsible_department")["timeline_deviation_days"].mean().sort_values(ascending=False).head(10)
    if not avg_delay.empty:
        plt.figure(figsize=(8, 4))
        avg_delay.plot(kind="barh", color="purple", edgecolor="black")
        plt.gca().invert_yaxis()
        plt.title("Average Timeline Deviation (Days) by Department")
        plt.xlabel("Days")
        plt.tight_layout()
        plt.show()

# --- 7Ô∏è‚É£ Export to Excel ---
output_path = Path("Automated_GL_Reconciliation_Report.xlsx")
with pd.ExcelWriter(output_path, engine="xlsxwriter") as writer:
    summary_df.to_excel(writer, sheet_name="Summary", index=False)
    df.to_excel(writer, sheet_name="Full_Data", index=False)
    df[df["working_status"] == "Pending"].to_excel(writer, sheet_name="Pending_Workings", index=False)
    df[df["timeline_status"] == "Delayed"].to_excel(writer, sheet_name="Timeline_Deviations", index=False)
    major_var_df.to_excel(writer, sheet_name="Major_Variances", index=False)

print(f"‚úÖ Report successfully exported ‚Üí {output_path}")

# ============================================
# üß© EXPORT AUGMENTED DATA TO CSV
# ============================================

from pathlib import Path

# Ensure the cleaned dataset is ready
augmented_df = main_df.copy()  # ‚úÖ Correct variable name

# Define safe export path
csv_path = Path("Augmented_GL_Reconciliation_Data.csv")

# Export to CSV (UTF-8 with index off)
augmented_df.to_csv(csv_path, index=False, encoding="utf-8-sig")

print(f"‚úÖ Augmented dataset successfully saved as ‚Üí {csv_path.resolve()}")
print(f"üìä Total Records: {len(augmented_df)} | Columns: {len(augmented_df.columns)}")

# Optional: show last few rows for confirmation
display(augmented_df.tail(3))

